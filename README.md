# Teaching STEM Undergraduates to Think Like the Web

## Context

Scientists and engineers invented electronic computers to accelerate
their work, but two generations on, most STEM researchers are still
not computationally competent: they do repetitive tasks manually
instead of automating them, develop software using a methodology best
summarized as "copy, paste, tweak, and pray", and fail to track their
work in any systematic, reproducible way.

Equally, while the World-Wide Web was created by a scientist to help
his peers share information, most still use it primarily as a way to
find and download PDFs.  Researchers --- particularly younger ones ---
might now believe that open data would fuel new insights, but don't
have the skills needed to create and provide a reusable data set.
Equally, any discussion of changing scientific publishing, improving
reproducibility, or using the web to support "science as a service"
eventually stumbles over a general lack of skills.

The general low level of computing skills among STEM researchers
persists for several reasons:

1.  *The curriculum is full.* Undergraduate STEM programs already
    struggle to cover material regarded as core to their field.  While
    everyone might agree that more material on programming,
    reproducible research, or web-enabled science would be desirable,
    no one can agree on what to take out to make room.

2.  *The blind leading the blind.* Most faculty do not have these
    skills themselves, and are therefore unable to pass them on.  (In
    addition, some do not see the need, arguing that since they got
    tenure without knowing what version control is, it clearly can't
    be *that* important.)

3.  *Difficulty of assessing impact.* It is easy to say, "This
    discovery could not have been made without use of that
    supercomputer."  It is much harder to attribute specific advances
    in science to prior training in general computing skills.

## Related Work

Several studies of how scientists use computers and the web (FIXME:
citation) have found that most scientists learn what they know about
these subjects through osmosis and word of mouth. Most training meant
to address this issue does not target scientists' specific needs,
places too much emphasis on programming and number crunching, and/or
jumps to advanced topics before scientists have mastered the basics.

Software Carpentry is the largest effort to date to address these
issues. Originally created as a training program at Los Alamos
National Laboratory in the late 1990s, it is now part of the Mozilla
Science Lab's efforts to help scientists get more done in less time
and with less pain.  Over 100 certified volunteer instructors
delivered two-day intensive workshops to more than 4200 people in 2013
alone, most covering:

*   the Unix shell (but really teaching people how to automate
    repetitive tasks);

*   version control with Git (but really showing how to track and share
    their work);

*   programming in Python or R (or rather, how to grow a program in a
    structured, modular, testable, reusable way); and

*   using SQL and databases (and what the difference is between
    structured and unstructured data).

Our curriculum and teaching practices are presently evidence-based: the
[training program](http://teaching.software-carpentry.org) we run for
instructors introduces participants to best practices such as peer
instruction, and to underlying concepts such as cognitive load theory
and Vygotsky's Zone of Proximal Development, and more specifically to
ongoing work by researchers including Guzdial & Ericson (Georgia
Tech), Simon (UC San Diego), and Sorva (Aalto University).  One
example of how we translate theory into practice is our near-exclusive
reliance on scientists as instructors, and on our insistence on live
coding during teaching as a way of demonstrating and transferring
authentic practice to learners.

In addition, we have been assessing retention and learning outcomes
since the beginning of our Sloan-funded in January 2012.  Work was
begun by Dr. Jorge Aranda (then at the University of Victoria), and is
now being done by Jory Schossau (Michigan State University) in
collaboration with Prof. Julie Libarkin (also MSU).  Schossau and is
using both qualitative and quantitative methods to determine which of
the concepts and practices we teach are being adopted, and how they
are changing scientists' work.

## Proposal

Software Carpentry primarily targets graduate students, post-docs, and
faculty.  The aims of this work are to:

1.  deliver training to STEM researchers earlier in their career ---
    specifically, to undergraduate students who are likely to go on to
    graduate studies and research careers;

2.  to assess the medium-term impact of that training on their work
    and their careers; and

3.  to use the results of that assessment to improve the training.

More specifically, we would like to run workshops for undergraduate
students taking part in the [NSF REU
program](http://www.nsf.gov/crssprgm/reu) at or near the start of
their time in the lab.  This training will help them be more
productive during their REU (graduate participants typically report
that what we teach saves them a day a week), and also prepare them to
work in a world where all aspects of science are increasingly
dependent on computing.

If this proposal is successful, we will:

1.  Run workshops at eight sites each year for four years, timed to
    coincide with the start of the summer REU influx, for a minimum of
    40 learners per site.  The six home sites for investigators named
    in this proposal (Michigan State U., Utah State U., George
    Washington U., U. New Mexico, UC Berkeley, and U. Wisconsin -
    Madison) will run workshops in each year of the program; two other
    sites will be added each year to increase the diversity of our
    study population.

2.  Expand our assessment efforts to compare REU student outcomes with
    those of other learners, to see what impact this training has on
    them compared with non-participant peers, and to assess and
    improve our instructional techniques.  More specifically, this
    proposal includes full-time funding for one post-doctoral
    researcher who will monitor participants in these workshops and
    participants in a selected subset of our regular (graduate-level)
    workshops for comparison purposes.

3.  Hire a professional videographer in years 1 and 2 to record and
    edit lectures so that we may make our core curriculum available
    under a Creative Commons license to anyone who wishes to use it
    for self-directed study or in conjunction with novel teaching
    techniques such as flipped classrooms.

4.  Hire one graduate student at each of the eight sites each year to
    provide part-time technical support to workshop participants, and
    to develop new "capstone" curricular materials aimed at specific
    disciplines.

## Participants

### Prof. Rachel Slaybaugh (University of California - Berkeley)

FIXME: bio

### Prof. [Lorena Barba](http://lorenabarba.com/) (George Washington University)

FIXME: bio

### Prof. [C. Titus Brown](http://ged.msu.edu/) (Michigan State University)

FIXME: bio

### Prof. [Scott Collins](http://temperate.lternet.edu/collins/) (University of New Mexico)

FIXME: bio

### Dr. Katy Huff (University of California - Berkeley)

FIXME: bio

### Dr. [Tracy Teal](http://idyll.org/~tracyt/) (Michigan State University)

FIXME: bio

### Prof. [Ethan White](http://whitelab.weecology.org/) (Utah State University)

FIXME: bio

### Dr. Greg Wilson (Mozilla Science Lab / Software Carpentry)

FIXME: bio

### Prof. [Paul Wilson](http://cnerg.github.io/) (University of Wisconsin - Madison)

FIXME: bio

## Budget

*   Bootcamps: 8 sites x 1 bootcamp/year x 4 years x $3K/bootcamp = $96K
*   Assessment (post-doc): $70K/year x 4 years = $280K
*   Graduate interns: $40K/year x 0.5 time x 4/12 month/year x 4 years x 6 sites = $160K
*   Video production: $30K
*   Coordination (Mozilla): $50K/year x 1/12 month/year x 4 years = $17K
*   TOTAL: $583K

Note 1: this is all *before* overheads.

Note 2: this funds a full-time assessment position.  We can cut $140K
from the budget by funding assessment half-time from this grant, and
looking to other sources (Sloan) for more funding; that gets us down
to $443K before overheads.
